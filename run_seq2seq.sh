#nohup python3 -u run_seq2seq.py --data_dir /data_slow/common/dataset/nlp/LCSTS_new --src_file train.json --model_type unilm --model_name_or_path /data_slow/common/pretrain/torch_unilm_model --output_dir data/unilm_lcsts/ --max_seq_length 512 --max_position_embeddings 512 --do_train --do_lower_case --train_batch_size 8 --gradient_accumulation_steps 4 --learning_rate 1e-5 --num_train_epochs 3 > data/log_train_lcsts.txt 2>&1 &

#nohup python3 -u run_seq2seq.py --data_dir /home/hanwei/data/dataset/nlp/user2query_list --src_file train.json --model_type unilm --model_name_or_path /data_slow/common/pretrain/torch_unilm_model --output_dir data/unilm_query2query_v5_user_hq/ --max_seq_length 512 --max_position_embeddings 512 --do_train --do_lower_case --train_batch_size 9 --learning_rate 1e-5 --num_train_epochs 3 > data/log_train.txt.user_hq 2>&1 &

nohup python3 -u  -m torch.distributed.launch --nproc-per-node 2 --nnodes 1 --node-rank 0 --master-addr "127.0.0.1" --master-port 12355 run_seq2seq.py --data_dir /data_slow/common/dataset/nlp/job_hunting/job2title --src_file train --src_desc jd --tgt_desc title --num_workers 20 --parallel --fp16_opt_level "01" --model_type unilm --model_name_or_path /data_slow/common/pretrain/torch_unilm_model --output_dir data/unilm_job2title/ --max_seq_length 512 --max_position_embeddings 512 --do_train --do_lower_case --train_batch_size 25 --gradient_accumulation_steps 1 --learning_rate 1e-5 --num_train_epochs 3 > data/log_train_job2title.txt 2>&1 &
